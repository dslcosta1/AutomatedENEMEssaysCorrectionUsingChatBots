{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d24035-4061-4e02-80e9-06be2b19c9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, cohen_kappa_score\n",
    "\n",
    "\n",
    "\n",
    "from utils.nilc_nlp import *\n",
    "from utils import dataset_setup\n",
    "from utils.save_results import *\n",
    "from utils.read_results import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4758aea-febd-419e-ba9d-d023d9ab609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_elements_by_text(dataset_A, dataset_B, text_column='essay_text'):\n",
    "    \"\"\"\n",
    "    Filtra os elementos do dataset A que possuem o mesmo `essay_text` presente no dataset B.\n",
    "    \n",
    "    Parameters:\n",
    "        dataset_A (pd.DataFrame): Dataset maior com a coluna `essay_text`.\n",
    "        dataset_B (pd.DataFrame): Dataset menor com a coluna `essay_text`.\n",
    "        text_column (str): Nome da coluna que contém os textos nos dois datasets.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Subconjunto do dataset A com os textos encontrados no dataset B.\n",
    "    \"\"\"\n",
    "    # Converte os textos do dataset B para um conjunto para busca rápida\n",
    "    texts_in_B = set(dataset_B[text_column])\n",
    "    \n",
    "    # Filtra o dataset A onde o texto está presente no conjunto de textos do B\n",
    "    filtered_A = dataset_A[dataset_A[text_column].isin(texts_in_B)]\n",
    "    \n",
    "    return filtered_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca45a275-c78f-4d07-bd80-dabefd474f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(predictions):\n",
    "    \"\"\"\n",
    "    Decode normalized predictions (0-5) to the original format (0, 40, 80, 120, 160, 200).\n",
    "    \n",
    "    Parameters:\n",
    "        predictions (list or numpy array): A list or array of normalized predictions (values 0-5).\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: Decoded predictions in the original format (values 0, 40, 80, 120, 160, 200).\n",
    "    \"\"\"\n",
    "    reverse_grade_mapping = {0: 0, 1: 40, 2: 80, 3: 120, 4: 160, 5: 200}\n",
    "    return np.vectorize(reverse_grade_mapping.get)(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a49efb00-015d-4f9e-b67d-c2dc482bf74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Data Preprocessing\n",
    "def process_y(y):\n",
    "    # Remove brackets, split by space, and take the first 5 values\n",
    "    y_processed = [list(map(int, label.strip(\"[]\").replace(\",\", \" \").split()[:5])) for label in y]\n",
    "    return np.array(y_processed)\n",
    "\n",
    "# Encoding class labels to integers\n",
    "def encode_classes(y):\n",
    "    encoders = [LabelEncoder() for _ in range(y.shape[1])]\n",
    "    y_encoded = np.array([encoders[i].fit_transform(y[:, i]) for i in range(y.shape[1])]).T\n",
    "    return y_encoded, encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05ecdd61-c3f9-426f-b60d-4f5469a1b14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map grades to normalized values\n",
    "grade_mapping = {0: 0, 40: 1, 80: 2, 120: 3, 160: 4, 200: 5}\n",
    "\n",
    "# Normalize y labels\n",
    "def normalize_y(y):\n",
    "    return np.vectorize(grade_mapping.get)(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16051b0c-87e7-41f6-9ca2-07d80f97dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20203679-3676-4bdc-b1c2-2b3b300e1f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset escolhido: propor2024\n",
      "Propor2024 train size = 744\n",
      "Propor2024 train and validation size = 960\n",
      "Propor2024 total size = 1155\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setup Dataset\n",
    "\"\"\"\n",
    "dataset_name, dataset_code = dataset_setup.setup_dataset(3)\n",
    "essays_dataset = dataset_setup.getDataset(path_to, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2e1feb1-e524-48d9-83bc-fca10b8bc3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nilc_metrix\"\n",
    "experiment_name = \"exp0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f53a9932-0e95-4a05-b55f-f75231580bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder './data/results/nilc_metrix' already exists.\n",
      "Folder './data/results/nilc_metrix/exp0' already exists.\n",
      "Folder './data/results/nilc_metrix/exp0/propor2024' already exists.\n",
      "CSV with the answer will be saved in: propor2024-nilc_metrix-exp0-0\n",
      "Folder './data/results/nilc_metrix' already exists.\n",
      "Folder './data/results/nilc_metrix/exp0' already exists.\n",
      "Folder './data/results/nilc_metrix/exp0/essaysFullGrade' already exists.\n",
      "CSV with the answer will be saved in: essaysFullGrade-nilc_metrix-exp0-0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Read Nilc-Metrix dataset \n",
    "\"\"\"\n",
    "\n",
    "dataset_name = \"propor2024\"\n",
    "path_to_save = create_experiment_folder(path_to, model_name, experiment_name, dataset_name)\n",
    "filename_to_save = build_filename_to_save(model_name, experiment_name, dataset_name, \"0\")\n",
    "dataset_propor = read_csv(path_to_save, filename_to_save)\n",
    "\n",
    "dataset_name = \"essaysFullGrade\"\n",
    "path_to_save = create_experiment_folder(path_to, model_name, experiment_name, dataset_name)\n",
    "filename_to_save = build_filename_to_save(model_name, experiment_name, dataset_name, \"0\")\n",
    "dataset_full_grade = read_csv(path_to_save, filename_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7defdafd-71ab-437b-8ea4-b0a4c8a8b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read Train, Test and Validation file\n",
    "\"\"\"\n",
    "dataset_base_path = \"./data/Datasets/\"\n",
    "\n",
    "path_to_propor = dataset_base_path + \"propor2024/\"\n",
    "train_dataset_propor = read_csv(path_to_propor, \"train\")\n",
    "validation_dataset_propor = read_csv(path_to_propor, \"validation\")\n",
    "test_dataset_propor = read_csv(path_to_propor, \"test\")\n",
    "\n",
    "path_to_dataset_full_grade = dataset_base_path + \"fullGradeEnemEssays2024/\"\n",
    "train_dataset_full_grade = read_csv(path_to_dataset_full_grade, \"train\")\n",
    "validation_dataset_full_grade = read_csv(path_to_dataset_full_grade, \"train\")\n",
    "test_dataset_full_grade = read_csv(path_to_dataset_full_grade, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86d90101-506f-4266-85fa-306d30d425eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_propor = filter_elements_by_text(dataset_propor, test_dataset_propor)\n",
    "test_full_grade = filter_elements_by_text(dataset_full_grade, test_dataset_full_grade)\n",
    "test_dataset = pd.concat([test_propor, test_full_grade], ignore_index=True)\n",
    "\n",
    "validation_propor = filter_elements_by_text(dataset_propor, validation_dataset_propor)\n",
    "validation_full_grade = filter_elements_by_text(dataset_full_grade, validation_dataset_full_grade)\n",
    "train_propor = filter_elements_by_text(dataset_propor, train_dataset_propor)\n",
    "train_full_grade = filter_elements_by_text(dataset_full_grade, train_dataset_full_grade)\n",
    "train_dataset = pd.concat([validation_propor, validation_full_grade, train_propor, train_full_grade], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7cc40314-b998-4989-970e-ed73515ba02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "train_dataset = train_dataset.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "\n",
    "columns_to_drop = ['author', 'source']\n",
    "X_train = train_dataset.iloc[:, 6:].drop(columns=columns_to_drop)\n",
    "y_train = normalize_y(process_y(train_dataset[\"grades\"]))\n",
    "X_test = test_dataset.iloc[:, 6:].drop(columns=columns_to_drop)\n",
    "y_test = normalize_y(process_y(test_dataset[\"grades\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c63721c4-a3cf-4752-b583-f645ae12d22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adjective_ratio</th>\n",
       "      <th>adverbs</th>\n",
       "      <th>content_words</th>\n",
       "      <th>flesch</th>\n",
       "      <th>function_words</th>\n",
       "      <th>sentences_per_paragraph</th>\n",
       "      <th>syllables_per_content_word</th>\n",
       "      <th>words_per_sentence</th>\n",
       "      <th>noun_ratio</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>...</th>\n",
       "      <th>lsa_all_std</th>\n",
       "      <th>lsa_paragraph_mean</th>\n",
       "      <th>lsa_paragraph_std</th>\n",
       "      <th>lsa_givenness_mean</th>\n",
       "      <th>lsa_givenness_std</th>\n",
       "      <th>lsa_span_mean</th>\n",
       "      <th>lsa_span_std</th>\n",
       "      <th>negative_words</th>\n",
       "      <th>positive_words</th>\n",
       "      <th>ratio_function_to_content_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.09677</td>\n",
       "      <td>0.03763</td>\n",
       "      <td>0.54839</td>\n",
       "      <td>26.86732</td>\n",
       "      <td>0.45161</td>\n",
       "      <td>1.25000</td>\n",
       "      <td>2.87255</td>\n",
       "      <td>37.20000</td>\n",
       "      <td>0.25269</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.94768</td>\n",
       "      <td>0.00200</td>\n",
       "      <td>0.93859</td>\n",
       "      <td>0.03030</td>\n",
       "      <td>0.94074</td>\n",
       "      <td>0.03158</td>\n",
       "      <td>0.30392</td>\n",
       "      <td>0.51961</td>\n",
       "      <td>0.82353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.07722</td>\n",
       "      <td>0.03475</td>\n",
       "      <td>0.56371</td>\n",
       "      <td>26.72788</td>\n",
       "      <td>0.43629</td>\n",
       "      <td>1.75000</td>\n",
       "      <td>2.90411</td>\n",
       "      <td>37.00000</td>\n",
       "      <td>0.30116</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00475</td>\n",
       "      <td>0.94982</td>\n",
       "      <td>0.00888</td>\n",
       "      <td>0.92336</td>\n",
       "      <td>0.02623</td>\n",
       "      <td>0.92598</td>\n",
       "      <td>0.02786</td>\n",
       "      <td>0.39041</td>\n",
       "      <td>0.64384</td>\n",
       "      <td>0.77397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11422</td>\n",
       "      <td>0.03233</td>\n",
       "      <td>0.57974</td>\n",
       "      <td>16.10474</td>\n",
       "      <td>0.42026</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>3.21933</td>\n",
       "      <td>29.00000</td>\n",
       "      <td>0.31034</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02973</td>\n",
       "      <td>0.96210</td>\n",
       "      <td>0.01033</td>\n",
       "      <td>0.92901</td>\n",
       "      <td>0.02870</td>\n",
       "      <td>0.94098</td>\n",
       "      <td>0.03241</td>\n",
       "      <td>0.27881</td>\n",
       "      <td>0.40892</td>\n",
       "      <td>0.72491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.06604</td>\n",
       "      <td>0.06604</td>\n",
       "      <td>0.59434</td>\n",
       "      <td>25.01083</td>\n",
       "      <td>0.40566</td>\n",
       "      <td>2.50000</td>\n",
       "      <td>2.96296</td>\n",
       "      <td>31.80000</td>\n",
       "      <td>0.31761</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03573</td>\n",
       "      <td>0.94256</td>\n",
       "      <td>0.01270</td>\n",
       "      <td>0.92729</td>\n",
       "      <td>0.02018</td>\n",
       "      <td>0.93454</td>\n",
       "      <td>0.01985</td>\n",
       "      <td>0.38095</td>\n",
       "      <td>0.57672</td>\n",
       "      <td>0.68254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06949</td>\n",
       "      <td>0.03625</td>\n",
       "      <td>0.54381</td>\n",
       "      <td>11.44846</td>\n",
       "      <td>0.45619</td>\n",
       "      <td>1.75000</td>\n",
       "      <td>3.00556</td>\n",
       "      <td>47.28571</td>\n",
       "      <td>0.28399</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02005</td>\n",
       "      <td>0.94850</td>\n",
       "      <td>0.01435</td>\n",
       "      <td>0.93875</td>\n",
       "      <td>0.02211</td>\n",
       "      <td>0.94432</td>\n",
       "      <td>0.02450</td>\n",
       "      <td>0.37778</td>\n",
       "      <td>0.63333</td>\n",
       "      <td>0.83889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192</th>\n",
       "      <td>0.08782</td>\n",
       "      <td>0.02266</td>\n",
       "      <td>0.54958</td>\n",
       "      <td>25.60644</td>\n",
       "      <td>0.45042</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>3.22165</td>\n",
       "      <td>22.06250</td>\n",
       "      <td>0.29745</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04475</td>\n",
       "      <td>0.96526</td>\n",
       "      <td>0.00474</td>\n",
       "      <td>0.91248</td>\n",
       "      <td>0.03783</td>\n",
       "      <td>0.92664</td>\n",
       "      <td>0.03326</td>\n",
       "      <td>0.38660</td>\n",
       "      <td>0.53093</td>\n",
       "      <td>0.81959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>0.09314</td>\n",
       "      <td>0.04902</td>\n",
       "      <td>0.60784</td>\n",
       "      <td>21.02559</td>\n",
       "      <td>0.39216</td>\n",
       "      <td>2.33333</td>\n",
       "      <td>2.96774</td>\n",
       "      <td>29.14286</td>\n",
       "      <td>0.24020</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07313</td>\n",
       "      <td>0.95407</td>\n",
       "      <td>0.01101</td>\n",
       "      <td>0.91885</td>\n",
       "      <td>0.06844</td>\n",
       "      <td>0.92214</td>\n",
       "      <td>0.06990</td>\n",
       "      <td>0.21774</td>\n",
       "      <td>0.58065</td>\n",
       "      <td>0.64516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>0.07527</td>\n",
       "      <td>0.04301</td>\n",
       "      <td>0.60215</td>\n",
       "      <td>10.67677</td>\n",
       "      <td>0.39785</td>\n",
       "      <td>1.40000</td>\n",
       "      <td>3.02976</td>\n",
       "      <td>39.85714</td>\n",
       "      <td>0.27957</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00690</td>\n",
       "      <td>0.92241</td>\n",
       "      <td>0.00967</td>\n",
       "      <td>0.91082</td>\n",
       "      <td>0.04010</td>\n",
       "      <td>0.92014</td>\n",
       "      <td>0.04215</td>\n",
       "      <td>0.20833</td>\n",
       "      <td>0.44048</td>\n",
       "      <td>0.66071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>0.09677</td>\n",
       "      <td>0.03763</td>\n",
       "      <td>0.54839</td>\n",
       "      <td>26.86732</td>\n",
       "      <td>0.45161</td>\n",
       "      <td>1.25000</td>\n",
       "      <td>2.87255</td>\n",
       "      <td>37.20000</td>\n",
       "      <td>0.25269</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.94768</td>\n",
       "      <td>0.00200</td>\n",
       "      <td>0.93859</td>\n",
       "      <td>0.03030</td>\n",
       "      <td>0.94074</td>\n",
       "      <td>0.03158</td>\n",
       "      <td>0.30392</td>\n",
       "      <td>0.51961</td>\n",
       "      <td>0.82353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>0.08805</td>\n",
       "      <td>0.05346</td>\n",
       "      <td>0.61006</td>\n",
       "      <td>25.27635</td>\n",
       "      <td>0.38994</td>\n",
       "      <td>3.25000</td>\n",
       "      <td>3.03608</td>\n",
       "      <td>24.46154</td>\n",
       "      <td>0.28931</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03806</td>\n",
       "      <td>0.96143</td>\n",
       "      <td>0.00493</td>\n",
       "      <td>0.92648</td>\n",
       "      <td>0.02794</td>\n",
       "      <td>0.93485</td>\n",
       "      <td>0.03079</td>\n",
       "      <td>0.23196</td>\n",
       "      <td>0.49485</td>\n",
       "      <td>0.63918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1197 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      adjective_ratio  adverbs  content_words    flesch  function_words  \\\n",
       "0             0.09677  0.03763        0.54839  26.86732         0.45161   \n",
       "1             0.07722  0.03475        0.56371  26.72788         0.43629   \n",
       "2             0.11422  0.03233        0.57974  16.10474         0.42026   \n",
       "3             0.06604  0.06604        0.59434  25.01083         0.40566   \n",
       "4             0.06949  0.03625        0.54381  11.44846         0.45619   \n",
       "...               ...      ...            ...       ...             ...   \n",
       "1192          0.08782  0.02266        0.54958  25.60644         0.45042   \n",
       "1193          0.09314  0.04902        0.60784  21.02559         0.39216   \n",
       "1194          0.07527  0.04301        0.60215  10.67677         0.39785   \n",
       "1195          0.09677  0.03763        0.54839  26.86732         0.45161   \n",
       "1196          0.08805  0.05346        0.61006  25.27635         0.38994   \n",
       "\n",
       "      sentences_per_paragraph  syllables_per_content_word  words_per_sentence  \\\n",
       "0                     1.25000                     2.87255            37.20000   \n",
       "1                     1.75000                     2.90411            37.00000   \n",
       "2                     4.00000                     3.21933            29.00000   \n",
       "3                     2.50000                     2.96296            31.80000   \n",
       "4                     1.75000                     3.00556            47.28571   \n",
       "...                       ...                         ...                 ...   \n",
       "1192                  4.00000                     3.22165            22.06250   \n",
       "1193                  2.33333                     2.96774            29.14286   \n",
       "1194                  1.40000                     3.02976            39.85714   \n",
       "1195                  1.25000                     2.87255            37.20000   \n",
       "1196                  3.25000                     3.03608            24.46154   \n",
       "\n",
       "      noun_ratio  paragraphs  ...  lsa_all_std  lsa_paragraph_mean  \\\n",
       "0        0.25269           4  ...      0.00000             0.94768   \n",
       "1        0.30116           4  ...      0.00475             0.94982   \n",
       "2        0.31034           4  ...      0.02973             0.96210   \n",
       "3        0.31761           4  ...      0.03573             0.94256   \n",
       "4        0.28399           4  ...      0.02005             0.94850   \n",
       "...          ...         ...  ...          ...                 ...   \n",
       "1192     0.29745           4  ...      0.04475             0.96526   \n",
       "1193     0.24020           3  ...      0.07313             0.95407   \n",
       "1194     0.27957           5  ...      0.00690             0.92241   \n",
       "1195     0.25269           4  ...      0.00000             0.94768   \n",
       "1196     0.28931           4  ...      0.03806             0.96143   \n",
       "\n",
       "      lsa_paragraph_std  lsa_givenness_mean  lsa_givenness_std  lsa_span_mean  \\\n",
       "0               0.00200             0.93859            0.03030        0.94074   \n",
       "1               0.00888             0.92336            0.02623        0.92598   \n",
       "2               0.01033             0.92901            0.02870        0.94098   \n",
       "3               0.01270             0.92729            0.02018        0.93454   \n",
       "4               0.01435             0.93875            0.02211        0.94432   \n",
       "...                 ...                 ...                ...            ...   \n",
       "1192            0.00474             0.91248            0.03783        0.92664   \n",
       "1193            0.01101             0.91885            0.06844        0.92214   \n",
       "1194            0.00967             0.91082            0.04010        0.92014   \n",
       "1195            0.00200             0.93859            0.03030        0.94074   \n",
       "1196            0.00493             0.92648            0.02794        0.93485   \n",
       "\n",
       "      lsa_span_std  negative_words  positive_words  \\\n",
       "0          0.03158         0.30392         0.51961   \n",
       "1          0.02786         0.39041         0.64384   \n",
       "2          0.03241         0.27881         0.40892   \n",
       "3          0.01985         0.38095         0.57672   \n",
       "4          0.02450         0.37778         0.63333   \n",
       "...            ...             ...             ...   \n",
       "1192       0.03326         0.38660         0.53093   \n",
       "1193       0.06990         0.21774         0.58065   \n",
       "1194       0.04215         0.20833         0.44048   \n",
       "1195       0.03158         0.30392         0.51961   \n",
       "1196       0.03079         0.23196         0.49485   \n",
       "\n",
       "      ratio_function_to_content_words  \n",
       "0                             0.82353  \n",
       "1                             0.77397  \n",
       "2                             0.72491  \n",
       "3                             0.68254  \n",
       "4                             0.83889  \n",
       "...                               ...  \n",
       "1192                          0.81959  \n",
       "1193                          0.64516  \n",
       "1194                          0.66071  \n",
       "1195                          0.82353  \n",
       "1196                          0.63918  \n",
       "\n",
       "[1197 rows x 72 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb7caa2a-6d6d-477e-a796-858585f88325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for c1...\n",
      "[4 3 5 ... 4 3 4]\n",
      "Accuracy for c1: 0.44\n",
      "QWK for c1: 0.38\n",
      "Training model for c2...\n",
      "[3 3 5 ... 4 5 4]\n",
      "Accuracy for c2: 0.42\n",
      "QWK for c2: 0.48\n",
      "Training model for c3...\n",
      "[3 2 5 ... 4 3 5]\n",
      "Accuracy for c3: 0.37\n",
      "QWK for c3: 0.55\n",
      "Training model for c4...\n",
      "[3 2 5 ... 3 3 4]\n",
      "Accuracy for c4: 0.51\n",
      "QWK for c4: 0.45\n",
      "Training model for c5...\n",
      "[1 0 5 ... 2 2 1]\n",
      "Accuracy for c5: 0.30\n",
      "QWK for c5: 0.42\n",
      "Final Predictions:\n",
      " [[3 3 4 4 3]\n",
      " [3 3 3 3 1]\n",
      " [3 3 1 3 0]\n",
      " ...\n",
      " [3 3 3 3 2]\n",
      " [3 3 2 3 1]\n",
      " [3 3 3 3 3]]\n"
     ]
    }
   ],
   "source": [
    "# Create and train a separate model for each competency\n",
    "models = {}\n",
    "predictions = {}\n",
    "accuracies = []\n",
    "\n",
    "best_params = {\n",
    "    'colsample_bytree': 1.0,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 7,\n",
    "    'n_estimators': 50,\n",
    "    'subsample': 0.6\n",
    "}\n",
    "\n",
    "for i in range(5):  # c1, c2, ..., c5\n",
    "    print(f\"Training model for c{i+1}...\")\n",
    "    \n",
    "    # Extract labels for the current competency\n",
    "    y_train_current = y_train[:, i]\n",
    "    print(y_train_current)\n",
    "    y_test_current = y_test[:, i]\n",
    "    \n",
    "    # Initialize XGBoost model\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective='multi:softmax',  # Multiclass classification\n",
    "        num_class=6,               # 6 classes: {0, 40, 80, 120, 160, 200}\n",
    "        eval_metric='mlogloss',    # Multiclass log loss\n",
    "        **best_params\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train_current)\n",
    "    \n",
    "    # Store the model\n",
    "    models[f\"c{i+1}\"] = model\n",
    "    \n",
    "    # Predict on the test set\n",
    "    predictions[f\"c{i+1}\"] = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate accuracy\n",
    "    accuracy = accuracy_score(y_test_current, predictions[f\"c{i+1}\"])\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Accuracy for c{i+1}: {accuracy:.2f}\")\n",
    "    qwk = cohen_kappa_score(y_test_current, predictions[f\"c{i+1}\"],  weights=\"quadratic\")\n",
    "    print(f\"QWK for c{i+1}: {qwk:.2f}\")\n",
    "\n",
    "# Combine all predictions\n",
    "final_predictions = np.column_stack([predictions[f\"c{i+1}\"] for i in range(5)])\n",
    "print(\"Final Predictions:\\n\", final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a5909bed-dfdf-4ae9-8787-9d109ab4ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "decodedPredictions = decode_predictions(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bb7e777f-99ce-4e01-939e-351d130b94f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[120, 120, 160, 160, 120],\n",
       "       [120, 120, 120, 120,  40],\n",
       "       [120, 120,  40, 120,   0],\n",
       "       ...,\n",
       "       [120, 120, 120, 120,  80],\n",
       "       [120, 120,  80, 120,  40],\n",
       "       [120, 120, 120, 120, 120]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decodedPredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f625118-f13b-43ac-b8b1-d8023b62ad8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "19379425-169d-4701-8226-9ad64cd1b90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gxboost\"\n",
    "dataset_name = \"extended2024\"\n",
    "experiment_name = \"exp1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd6c3c1e-75a6-4ae1-b87e-6d9b594486ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = []\n",
    "for decodedPrediction in decodedPredictions:\n",
    "    final_predictions.append(str(decodedPrediction))\n",
    "\n",
    "\n",
    "test_dataset[model_name + \"_grades\"] = final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "35141c08-392a-4aaf-9599-4e1f5e1606b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder './data/results/gxboost' already exists.\n",
      "Folder './data/results/gxboost/exp1' created.\n",
      "Folder './data/results/gxboost/exp1/extended2024' created.\n",
      "CSV with the answer will be saved in: extended2024-gxboost-exp1-1\n"
     ]
    }
   ],
   "source": [
    "path_to_save = create_experiment_folder(path_to, model_name, experiment_name, dataset_name)\n",
    "filename_to_save = build_filename_to_save(model_name, experiment_name, dataset_name, \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc65635a-dac0-4b20-9878-81338d6281d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.to_csv(path_to_save + \"/\" + filename_to_save + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3fb5a6-a66c-42db-8031-097398040723",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
